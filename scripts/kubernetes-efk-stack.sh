# EFK - elasticsearch fluentd kibana stack
# =============
EFK_NS="elastic-stack"
kubectl create namespace ${EFK_NS}

# best practice is to be explicit, don't change default namespace when installing
# kubectl config set-context --current --namespace=${EFK_NS}
# verify namespace change
# kubectl config view | grep 'namespace:'

EFK_CONFIG_DIR="${HOME}/.kube/install/efk"
md "${EFK_CONFIG_DIR}"

# stable/elastic-stack
# ====================
# NOT WORKING
# https://www.linode.com/docs/kubernetes/how-to-deploy-the-elastic-stack-on-kubernetes/
# helm install elastic-stack stable/elastic-stack

# elastic
# =======
helm repo add elastic https://helm.elastic.co
helm repo update

# logstash helm not compatible with helm 3, disabled for now
# helm install logstash elastic/logstash --set antiAffinity="soft" \
#   -n=${EFK_NS}

helm install elasticsearch elastic/elasticsearch -n=${EFK_NS} \
  --set antiAffinity="soft"

tee "${EFK_CONFIG_DIR}/elastic/kibana.yaml" >/dev/null <<EOF
# Allows you to add any config files in /usr/share/kibana/config/
kibanaConfig:
  kibana.yml: |
    # WARNING: THIS USED TO BE AN AUTOGENERATED SECTION
    # Default Kibana configuration for docker target
    server.name: "kibana"
    server.host: "0.0.0.0"
    elasticsearch.hosts: [ "http://elasticsearch:9200" ]
    xpack.monitoring.ui.container.elasticsearch.enabled: true

    # proxy compatibility, mounting at other point than root
    server.basePath: "/elastic-stack/kibana"
    server.rewriteBasePath: true
EOF
helm install kibana elastic/kibana -n=${EFK_NS} \
  -f "${EFK_CONFIG_DIR}/elastic/kibana.yaml"

# to restart kibana
kubectl rollout restart deployment kibana-kibana -n=${EFK_NS}

# ACTION: wait until the kibana pod exists
# KIBANA_POD=$(kubectl get pods -n=${EFK_NS} | grep kibana | awk '{print $1}')
KIBANA_POD=$(
  kubectl get pods \
    -n=${EFK_NS} \
    --selector app=kibana \
    -o jsonpath="{.items[0].metadata.name}"
)
kubectl port-forward ${KIBANA_POD} 5601:5601 -n=${EFK_NS} &

# bitnami
# =======

helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

tee "${EFK_CONFIG_DIR}/bitnami/elasticsearch-kibana.yaml" >/dev/null <<EOF
kibana:
  elasticsearch:
    hosts:
      - '{{ include "elasticsearch.coordinating.fullname" . }}'
    port: 9200

    extraConfiguration:
      # proxy compatibility, mounting at other point than root
      # added to /opt/bitnami/kibana/conf/kibana.yml
      "server.basePath": "/elastic-stack/kibana"
      "server.rewriteBasePath": true
EOF
tee "${EFK_CONFIG_DIR}/bitnami/elasticsearch-curator-actions.yaml" >/dev/null <<EOF
---
actions:
  1:
    action: delete_indices
    description: "Clean up ES by deleting old indices"
    options:
      timeout_override:
      continue_if_exception: False
      disable_action: False
      ignore_empty_list: True
    filters:
    - filtertype: age
      source: name
      direction: older
      timestring: '%Y.%m.%d'
      unit: days
      # default unit_count is 90
      unit_count: 7
      field:
      stats_result:
      epoch:
      exclude: False
EOF
# TODO: !!! --set curator.configMaps.action_file_yml needs the contents of the config file, no the path !!!
# solved temporarily by updating the cm manually:
#   k edit cm elasticsearch-elasticsearch-curator
helm install elasticsearch bitnami/elasticsearch -n=${EFK_NS} \
  --set master.persistence.size=10Gi \
  --set data.persistence.size=40Gi \
  --set global.kibanaEnabled=true \
  -f "${EFK_CONFIG_DIR}/bitnami/elasticsearch-kibana.yaml" \
  --set ingest.enabled=true \
  --set metrics.enabled=true \
  --set curator.enabled=true \
  --set curator.configMaps.action_file_yml="${EFK_CONFIG_DIR}/bitnami/elasticsearch-curator-actions.yaml"
#  --set data.resources='{ limits: { cpu: 1000m, memory: 2Gi }, requests: { cpu: 1000m, memory: 2Gi } }' \
#  --set master.resources='{ limits: { cpu: 1000m, memory: 2Gi }, requests: { cpu: 1000m, memory: 2Gi } }' \

# to verify all installed chart values
helm get all elasticsearch -n=${EFK_NS}

ES_SERVICE="elasticsearch-coordinating-only.${EFK_NS}.svc.cluster.local"
ES_HOSTNAME="elasticsearch-elasticsearch-coordinating-only"
ES_PORT=9200
tee "${EFK_CONFIG_DIR}/bitnami/kibana.yaml" >/dev/null <<EOF
# http:// is added automatically to hosts
elasticsearch.hosts: [${ES_HOSTNAME}]
elasticsearch.port: ${ES_PORT}

extraConfiguration:
  # proxy compatibility, mounting at other point than root
  # added to /opt/bitnami/kibana/conf/kibana.yml
  "server.basePath": "/elastic-stack/kibana"
  "server.rewriteBasePath": true
EOF
# fails unless hostnames and port are specified explicitly with set
helm install kibana bitnami/kibana -n=${EFK_NS} \
  --set "elasticsearch.hosts[0]=${ES_HOSTNAME}" \
  --set "elasticsearch.port=${ES_PORT}" \
  -f "${EFK_CONFIG_DIR}/bitnami/kibana.yaml"

# to enable base path:
# kubectl edit cm elasticsearch-kibana-conf
#   elasticsearch.hosts: [${ES_HOSTNAME}]
#   elasticsearch.port: ${ES_PORT}
#
# kubectl edit deployments.apps elasticsearch-kibana
#   spec.template.spec.containers[env].livenessProbe.heepGet.path=/elastic-stack/kibana/app/kibana
#   spec.template.spec.containers[env].readinessProbe.heepGet.path=/elastic-stack/kibana/app/kibana


#KIBANA_POD=$(
#  kubectl get pods \
#    -n=${EFK_NS} \
#    --selector app.kubernetes.io/name=kibana \
#    -o jsonpath="{.items[0].metadata.name}"
#)
#kubectl port-forward ${KIBANA_POD} --address 0.0.0.0 5601:5601 -n=${EFK_NS} &
##kubectl port-forward svc/elasticsearch-kibana --address 0.0.0.0 8080:80 -n=${EFK_NS} &
#kubectl port-forward svc/elasticsearch-elasticsearch-coordinating-only 9200:9200 -n=${EFK_NS} &

# https://github.com/bitnami/charts/tree/master/bitnami/fluentd
tee "${EFK_CONFIG_DIR}/bitnami/fluentd-elasticsearch-configmap.yaml" >/dev/null <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-elasticsearch-output
data:
  fluentd.conf: |
    {{- if .Values.metrics.enabled -}}
    # Prometheus Exporter Plugin
    # input plugin that exports metrics
    <source>
      @type prometheus
      port {{ .Values.metrics.service.port }}
    </source>

    # input plugin that collects metrics from MonitorAgent
    <source>
      @type prometheus_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>

    # input plugin that collects metrics for output plugin
    <source>
      @type prometheus_output_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>
    {{- end }}

    # Ignore fluentd own events
    <match fluent.**>
      @type null
    </match>

    # TCP input to receive logs from the forwarders
    <source>
      @type forward
      bind 0.0.0.0
      port {{ .Values.aggregator.port }}
    </source>

    # HTTP input for the liveness and readiness probes
    <source>
      @type http
      bind 0.0.0.0
      port 9880
    </source>

    # Throw the healthcheck to the standard output instead of forwarding it
    <match fluentd.healthcheck>
      @type stdout
    </match>

    # Send the logs to elasticsearch
    <match **>
      @type elasticsearch
      include_tag_key true
      host ${ES_SERVICE}
      port ${ES_PORT}
      logstash_format true
      logstash_prefix logstash

      <buffer>
        @type file
        path /opt/bitnami/fluentd/logs/buffers/logs.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever true
        retry_max_interval 30
        chunk_limit_size 2M
        queue_limit_length 8
        overflow_action block
      </buffer>
    </match>
EOF
helm install fluentd bitnami/fluentd -n=${EFK_NS} \
  -f "${EFK_CONFIG_DIR}/bitnami/fluentd-elasticsearch-configmap.yaml" \
  --set metrics.enabled=true \
  --set aggregator.configMap=fluentd-elasticsearch-output

kubectl port-forward fluentd-0 --address 0.0.0.0 24224:24224 -n=${EFK_NS} &

# kiwigrid
# ========

helm repo add kiwigrid https://kiwigrid.github.io
helm repo update

helm install fluentd-elasticsearch kiwigrid/fluentd-elasticsearch -n=${EFK_NS} \
  --set elasticsearch.host=elasticsearch-master


spec:
  clusterIP: 10.108.192.13
  externalTrafficPolicy: Cluster
  ports:
  - name: status-port
    nodePort: 32326
    port: 15020
    protocol: TCP
    targetPort: 15020
  - name: http2
    nodePort: 31566
    port: 80
    protocol: TCP
    targetPort: 80
  - name: https
    nodePort: 32534
    port: 443
    protocol: TCP
    targetPort: 443
  - name: kiali
    nodePort: 31613
    port: 15029
    protocol: TCP
    targetPort: 15029
  - name: prometheus
    nodePort: 32483
    port: 15030
    protocol: TCP
    targetPort: 15030
  - name: grafana
    nodePort: 31082
    port: 15031
    protocol: TCP
    targetPort: 15031
  - name: tracing
    nodePort: 31991
    port: 15032
    protocol: TCP
    targetPort: 15032
  - name: tls
    nodePort: 32062
    port: 15443
    protocol: TCP
    targetPort: 15443
  - name: tcp
    nodePort: 32010
    port: 31400
    protocol: TCP
    targetPort: 31400
  - name: elastic-stack-fluentd
    port: 24224
    protocol: TCP
    targetPort: 24224



apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: elastic-stack-bitnami-gateway
spec:
  selector:
    istio: ingressgateway # use Istio default gateway implementation
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: elastic-stack-bitnami-virtualservice
spec:
  hosts:
  - "*"
  gateways:
  - elastic-stack-bitnami-gateway
  http:
  - match:
    - uri:
        prefix: "/elastic-stack/kibana"
    route:
    - destination:
        port:
          number: 80
        host: elasticsearch-kibana.elastic-stack.svc.cluster.local

---
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: elastic-stack-bitnami-fluentd-gateway
spec:
  selector:
    istio: ingressgateway # use Istio default gateway implementation
  servers:
  - port:
      number: 24224
      name: tcp
      protocol: TCP
    hosts:
    - "*"
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: elastic-stack-bitnami-fluentd-virtualservice
spec:
  hosts:
  - "*"
  gateways:
  - elastic-stack-bitnami-fluentd-gateway
  tcp:
  - match:
    - port: 24224
    route:
    - destination:
        port:
          number: 24224
        host: fluentd-aggregator.elastic-stack.svc.cluster.local




apiVersion: v1
kind: Pod
metadata:
  name: dapi-envars-fieldref
spec:
  containers:
    - name: test-container
      image: k8s.gcr.io/busybox
      command: [ "sh", "-c"]
      args:
      - while true; do
          echo -en '\n';
          printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE;
          printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT;
          sleep 10;
        done;
      env:
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: MY_POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        - name: MY_NODE_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
  restartPolicy: Never


